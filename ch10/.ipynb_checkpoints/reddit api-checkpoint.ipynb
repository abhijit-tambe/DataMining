{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLIENT_ID = \"0eZo-Z0nv8M0uw\" \n",
    "CLIENT_SECRET = \"jn0jrTlgJ-yhYh0yyBAcVFBzc9g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USER_AGENT = \"python:<test0.1> (by /u/abhijit)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USERNAME = \"abhijit-tambe\" \n",
    "PASSWORD = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def login(username, password):\n",
    "    if password is None:\n",
    "        password = getpass.getpass(\"Enter reddit password for user {}: \".format(username))    \n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    # Setup an auth object with our credentials\n",
    "    client_auth = requests.auth.HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET)\n",
    "    # Make a post request to the access_token endpoint\n",
    "    post_data = {\"grant_type\": \"password\", \"username\": username, \"password\": password}\n",
    "    response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth,     \n",
    "                             data=post_data, headers=headers) \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = login(USERNAME, PASSWORD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': '179517556424-qtvHrtkLTlWE79ZArhHc-2y2YTs',\n",
       " 'expires_in': 3600,\n",
       " 'scope': '*',\n",
       " 'token_type': 'bearer'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddit = \"worldnews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://oauth.reddit.com/r/{}\".format(subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": \"bearer {}\".format(token['access_token']), \n",
    "\"User-Agent\": USER_AGENT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Korea legalize abortion after 66 year ban\n",
      "Fecal transplants result in massive long-term reduction in autism symptoms\n",
      "Declassified documents show CIA knew of secret Latin assassin unit - With the blessing of a CIA bent on thwarting Soviet expansion, South American military juntas together formed a special unit charged with going to France and elsewhere abroad to exterminate leftist opposition leaders\n",
      "Oil-eating bacteria discovered at the bottom of the Mariana Trench\n",
      "Chomsky: Trump Radically Interfered with Israel’s Election to Help Re-elect Netanyahu\n",
      "Man wearing Trump T-shirt admits abusing worshippers at New Zealand mosque\n",
      "'Extraordinary' 500-year-old library catalogue reveals books lost to time\n",
      "‘Attenborough effect’ leads to 53% drop in single use plastic in 12 months\n",
      "Brazilian president raises eyebrows by saying the Holocaust can be forgiven\n",
      "One in five cases of childhood asthma tied to pollution from cars and trucks, global study finds\n",
      "Chinese Hacking Steals Billions; U.S. Businesses Turn A Blind Eye\n",
      "Babies born out of wedlock are 'abandoned on streets of Saudi Arabia' due to fears of punishment\n",
      "Assassination fears as 2 top Putin officials die in suspicious circumstances\n",
      "Putin congratulates Kim Jong-un on re-election in N.Korea\n",
      "Steve Bannon ‘told Italy’s populist leader: Pope Francis is the enemy’ | Trump’s ex-strategist advised Matteo Salvini ‘to target pontiff’s stance on plight of refugees’\n",
      "'Shut the country down': British climate group Extinction Rebellion heads to US - With dozens of events next week, many hope arrival of climate punks who’ve swept the UK will be a watershed moment\n",
      "Rachel Maddow draws a straight line from Wikileaks to Don Trump Jr after Assange arrest\n",
      "Gene-silencing: 'New class' of medicine reverses disease porphyria\n",
      "British Columbia introduces law to require cars, trucks sold by 2040 be zero emission - Legislation aims to phase out gas-powered vehicles\n",
      "Chinese social media users are voicing their outrage after the prosecution of an American who stole a thumb from a Chinese Terracotta Warrior ended in a mistrial\n",
      "Bering Sea changes startle scientists, worry residents: \"The projections were saying we would've hit situations similar to what we saw last year, but not for another 40 or 50 years\"\n",
      "Law enforcement and security officials reveal that US leaders and executives allowed companies to be hacked by China without consequences because they were worried about confrontation causing trade and stock prices to drop.\n",
      "Brazil president raises eyebrows saying Holocaust can be forgiven\n",
      "Mueller report: Trump avoided interview with special counsel during Russia investigation. Trump lawyers allegedly decline request for interview as recently as January this year\n",
      "Law enforcement taps Google's Sensorvault for location data, report says\n"
     ]
    }
   ],
   "source": [
    "for story in result['data']['children']: \n",
    "    print(story['data']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links(subreddit, token, n_pages=5):\n",
    "    stories = []\n",
    "    after = None\n",
    "    for page_number in range(n_pages):\n",
    "        # Sleep before making calls to avoid going over the API limit\n",
    "        sleep(2)\n",
    "        # Setup headers and make call, just like in the login function\n",
    "        headers = {\"Authorization\": \"bearer {}\".format(token['access_token']), \"User-Agent\": USER_AGENT} \n",
    "        url = \"https://oauth.reddit.com/r/{}?limit=100\". format(subreddit)\n",
    "        if after:\n",
    "            # Append cursor for next page, if we have one\n",
    "            url += \"&after={}\".format(after)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        result = response.json()\n",
    "        # Get the new cursor for the next loop\n",
    "        after = result['data']['after']\n",
    "        # Add all of the news items to our stories list\n",
    "        for story in result['data']['children']:\n",
    "            stories.append((story['data']['title'], story['data']['url'], story['data']['score']))\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stories = get_links(\"worldnews\", token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "data_folder = os.path.join(os.path.expanduser(\"~\"),\"Datamining\",\"chapter10\",\"Data\", \"websites\", \"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_errors = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for title, url, score in stories:\n",
    "    output_filename = hashlib.md5(url.encode()).hexdigest() \n",
    "    fullpath = os.path.join(data_folder, output_filename + \".txt\")\n",
    "    try: \n",
    "        response = requests.get(url) \n",
    "        data = response.text \n",
    "        with open(fullpath, 'w') as outf: \n",
    "            outf.write(data)\n",
    "    except Exception as e:\n",
    "        number_errors += 1\n",
    "        # You can use this to view the errors, if you are getting too many:\n",
    "        #raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [os.path.join(data_folder, filename) for filename in os.listdir(data_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_output_folder = os.path.join(os.path.expanduser(\"~\"),\"Datamining\",\"chapter10\",\"Data\", \"websites\", \"textonly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skip_node_types = [\"script\", \"head\", \"style\", etree.Comment]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = etree.HTMLParser()\n",
    "\n",
    "def get_text_from_file(filename):\n",
    "    with open(filename) as inf:\n",
    "        html_tree = etree.parse(inf, parser) \n",
    "    return get_text_from_node(html_tree.getroot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_from_node(node):\n",
    "    try:\n",
    "        if len(node) == 0: \n",
    "        # No children, just return text from this item\n",
    "            if node.text: \n",
    "                return node.text \n",
    "            else:\n",
    "                return \"\"\n",
    "        else:\n",
    "            # This node has children, return the text from it:\n",
    "            results = (get_text_from_node(child)\n",
    "                       for child in node\n",
    "                       if child.tag not in skip_node_types)\n",
    "        result = str.join(\"\\n\", (r for r in results if len(r) > 1))\n",
    "        if len(result) >= 100:\n",
    "            return result\n",
    "        else:\n",
    "            return \"\"\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(data_folder):\n",
    "    text = get_text_from_file(os.path.join(data_folder, filename)) \n",
    "    with open(os.path.join(text_output_folder, filename), 'w' , encoding='utf-8') as outf: \n",
    "        outf.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "n_clusters = 10 \n",
    "pipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4)),\n",
    "                                     ('clusterer', KMeans(n_clusters=n_clusters)) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = [open(os.path.join(text_output_folder, filename), encoding='utf-8').read() for filename in os.listdir(text_output_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline.fit(documents)\n",
    "labels = pipeline.predict(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 contains 138 samples\n",
      "Cluster 1 contains 5 samples\n",
      "Cluster 2 contains 2 samples\n",
      "Cluster 3 contains 1 samples\n",
      "Cluster 4 contains 7 samples\n",
      "Cluster 5 contains 2 samples\n",
      "Cluster 6 contains 1 samples\n",
      "Cluster 7 contains 2 samples\n",
      "Cluster 8 contains 2 samples\n",
      "Cluster 9 contains 1 samples\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(labels) \n",
    "for cluster_number in range(n_clusters): \n",
    "    print(\"Cluster {} contains {} samples\".format(cluster_number, c[cluster_number]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.51385148898275"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.named_steps['clusterer'].inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inertia_scores = [] \n",
    "n_cluster_values = list(range(2, 20)) \n",
    "for n_clusters in n_cluster_values: \n",
    "    cur_inertia_scores = [] \n",
    "    X = TfidfVectorizer(max_df=0.4).fit_transform(documents) \n",
    "    for i in range(10): \n",
    "        km = KMeans(n_clusters=n_clusters).fit(X) \n",
    "        cur_inertia_scores.append(km.inertia_) \n",
    "        inertia_scores.append(cur_inertia_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters = 6 \n",
    "pipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4)),\n",
    "                     ('clusterer', KMeans(n_clusters=n_clusters)) ])\n",
    "pipeline.fit(documents) \n",
    "labels = pipeline.predict(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = pipeline.named_steps['feature_extraction'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 contains 145 samples\n",
      " Most important terms\n",
      " 1) we (score: 0.0219)\n",
      " 2) by (score: 0.0213)\n",
      " 3) he (score: 0.0213)\n",
      " 4) have (score: 0.0205)\n",
      " 5) they (score: 0.0180)\n",
      "Cluster 1 contains 7 samples\n",
      " Most important terms\n",
      " 1) cache (score: 0.4499)\n",
      " 2) found (score: 0.3069)\n",
      " 3) ewr (score: 0.2317)\n",
      " 4) varnish (score: 0.2317)\n",
      " 5) 404 (score: 0.2317)\n",
      "Cluster 2 contains 1 samples\n",
      " Most important terms\n",
      " 1) electric (score: 0.4193)\n",
      " 2) australia (score: 0.3478)\n",
      " 3) vehicles (score: 0.2375)\n",
      " 4) australians (score: 0.1880)\n",
      " 5) voters (score: 0.1732)\n",
      "Cluster 3 contains 2 samples\n",
      " Most important terms\n",
      " 1) trump (score: 0.2739)\n",
      " 2) kim (score: 0.2718)\n",
      " 3) korea (score: 0.2582)\n",
      " 4) north (score: 0.2276)\n",
      " 5) summit (score: 0.1931)\n",
      "Cluster 4 contains 5 samples\n",
      " Most important terms\n",
      " 1) police (score: 0.3267)\n",
      " 2) car (score: 0.2714)\n",
      " 3) officers (score: 0.2596)\n",
      " 4) embassy (score: 0.1958)\n",
      " 5) ambassador (score: 0.1570)\n",
      "Cluster 5 contains 1 samples\n",
      " Most important terms\n",
      " 1) sudan (score: 0.2998)\n",
      " 2) civilian (score: 0.1998)\n",
      " 3) burhan (score: 0.1998)\n",
      " 4) al (score: 0.1989)\n",
      " 5) military (score: 0.1709)\n"
     ]
    }
   ],
   "source": [
    "for cluster_number in range(n_clusters): \n",
    "    print(\"Cluster {} contains {} samples\".format(cluster_number, c[cluster_number]))\n",
    "    print(\" Most important terms\")\n",
    "    centroid = pipeline.named_steps['clusterer'].cluster_centers_[cluster_number]\n",
    "    most_important = centroid.argsort()\n",
    "    for i in range(5):\n",
    "        term_index = most_important[-(i+1)]\n",
    "        print(\" {0}) {1} (score: {2:.4f})\".format(i+1, terms[term_index], centroid[term_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pipeline.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_coassociation_matrix(labels):\n",
    "    rows = [] \n",
    "    cols = []\n",
    "    unique_labels = set(labels) \n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        for index1 in indices:\n",
    "            for index2 in indices:\n",
    "                rows.append(index1)\n",
    "                cols.append(index2)\n",
    "    data = np.ones((len(rows),)) \n",
    "    return csr_matrix((data, (rows, cols)), dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = create_coassociation_matrix(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<161x161 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 21105 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import minimum_spanning_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mst = minimum_spanning_tree(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mst = minimum_spanning_tree(-C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline.fit(documents) \n",
    "labels2 = pipeline.predict(documents) \n",
    "C2 = create_coassociation_matrix(labels2) \n",
    "C_sum = (C + C2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mst = minimum_spanning_tree(-C_sum) \n",
    "mst.data[mst.data > -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import connected_components \n",
    "number_of_clusters, labels = connected_components(mst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClusterMixin \n",
    "class EAC(BaseEstimator, ClusterMixin):\n",
    "    def __init__(self, n_clusterings=10, cut_threshold=0.5, n_clusters_range=(3, 10)): \n",
    "        self.n_clusterings = n_clusterings\n",
    "        self.cut_threshold = cut_threshold\n",
    "        self.n_clusters_range = n_clusters_range\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        C = sum((create_coassociation_matrix(self._single_clustering(X))\n",
    "                 for i in range(self.n_clusterings)))\n",
    "        mst = minimum_spanning_tree(-C)\n",
    "        mst.data[mst.data > -self.cut_threshold] = 0\n",
    "        mst.eliminate_zeros()\n",
    "        self.n_components, self.labels_ = connected_components(mst)\n",
    "        return self\n",
    " \n",
    "    def _single_clustering(self, X):\n",
    "        n_clusters = np.random.randint(*self.n_clusters_range)\n",
    "        km = KMeans(n_clusters=n_clusters)\n",
    "        return km.fit_predict(X)\n",
    " \n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4)),\n",
    "                     ('clusterer', EAC()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(max_df=0.4) \n",
    "X = vec.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans \n",
    "mbkm = MiniBatchKMeans(random_state=14, n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10 \n",
    "for iteration in range(int(X.shape[0] / batch_size)): \n",
    "    start = batch_size * iteration \n",
    "    end = batch_size * (iteration + 1) \n",
    "    mbkm.partial_fit(X[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = mbkm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PartialFitPipeline(Pipeline):\n",
    "    def partial_fit(self, X, y=None):\n",
    "        Xt = X\n",
    "        for name, transform in self.steps[:-1]:\n",
    "            Xt = transform.transform(Xt)\n",
    "        return self.steps[-1][1].partial_fit(Xt, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "pipeline = PartialFitPipeline([('feature_extraction', HashingVectorizer()),\n",
    "                               ('clusterer', MiniBatchKMeans(random_state=14, n_clusters=3)) ])\n",
    "batch_size = 10 \n",
    "for iteration in range(int(len(documents) / batch_size)): \n",
    "    start = batch_size * iteration \n",
    "    end = batch_size * (iteration + 1)\n",
    "    pipeline.partial_fit(documents[start:end]) \n",
    "labels = pipeline.predict(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
